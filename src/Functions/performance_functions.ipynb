{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c06110-eff9-41ab-8db6-619171b81655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd34259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_weeks_forecast(df, nsteps, n_week_sum):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses historic data to compare how an aggregated n week out forecast compares to\n",
    "    the actual historic aggregates, given we have m < n pieces of data already.\n",
    "    For example, by setting nsteps = 1, and n_week_sum = 4, we are comparing 4\n",
    "    week aggregates against an aggregated forecast over same 4 weeks, whereby only the\n",
    "    last 1 week is actually forecast out. The previous 3 weeks are assumed known.\n",
    "    This function allows us to see how an aggregated 4 week out forecast will improve\n",
    "    over time with each new week of data that comes in.\n",
    "\n",
    "    INPUTS:\n",
    "    ----------------\n",
    "        df (pandas.core.frame.DataFrame) : A dataframe of the actual time series values\n",
    "        aswell as a column of values which have been forecast out nsteps ahead by one of\n",
    "        our models. Such a dataframe is returned by each of our models.\n",
    "\n",
    "        nsteps (int) : The number of steps ahead we are forecasting out.\n",
    "\n",
    "        n_week_sum (int) : The number of weeks we aggregate over.\n",
    "\n",
    "    RETURNS:\n",
    "    ----------------\n",
    "        (pandas.core.frame.DataFrame): A dataframe with a column of historic aggregated\n",
    "        values and a column of forecasts.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ts_dummy = df.copy()\n",
    "    date_col = ts_dummy.columns.values.tolist()[0]\n",
    "    series_col = ts_dummy.columns.values.tolist()[1]\n",
    "    pred_col = ts_dummy.columns.values.tolist()[2]\n",
    "\n",
    "    cols = [\"forecast_end_date\", \"actual_\" + str(n_week_sum) + \"_week_sum\", pred_col]\n",
    "    forecast_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # get the first index where the predicted data starts at\n",
    "    final_training_index = ts_dummy[pred_col].first_valid_index() - 1\n",
    "    first_forecast_index = final_training_index + 1\n",
    "\n",
    "    while (final_training_index + nsteps) <= (len(ts_dummy) - 1):\n",
    "\n",
    "        predicted_value = 0\n",
    "        actual_value = 0\n",
    "        look_back = n_week_sum - nsteps\n",
    "\n",
    "        # if we make a 2 step ahead forcast on on a date D, then the actual 4 week total \n",
    "        #is D-1, D, D+1 and D+2.\n",
    "        for i in range(look_back):\n",
    "            predicted_value = (\n",
    "                predicted_value + ts_dummy[series_col].iloc[final_training_index - i]\n",
    "            )\n",
    "            actual_value = (\n",
    "                actual_value + ts_dummy[series_col].iloc[final_training_index - i]\n",
    "            )\n",
    "        for j in range(nsteps):\n",
    "            predicted_value = (\n",
    "                predicted_value + ts_dummy[pred_col].iloc[first_forecast_index + j]\n",
    "            )\n",
    "            actual_value = (\n",
    "                actual_value + ts_dummy[series_col].iloc[first_forecast_index + j]\n",
    "            )\n",
    "\n",
    "        # Create a new dataframe of the forecasts\n",
    "        actual_end_date = ts_dummy[date_col].iloc[final_training_index + nsteps]\n",
    "        vals = [actual_end_date, actual_value, predicted_value]\n",
    "        # forecast_df = forecast_df.append(dict(zip(cols,vals)), ignore_index = True)\n",
    "        forecast_df = pd.concat(\n",
    "            [forecast_df, pd.DataFrame.from_records([dict(zip(cols, vals))])],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        final_training_index = final_training_index + nsteps\n",
    "        first_forecast_index = final_training_index + 1\n",
    "\n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af50ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(df, performance_lag, summarise=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes performance metrics (mae, mape, rmse) for the historic forecasts of a time series.\n",
    "\n",
    "    INPUTS:\n",
    "    ----------------\n",
    "        df (pandas.core.frame.DataFrame) : A dataframe of the actual time series values\n",
    "        aswell as a column of values which have been forecast by one of our models.\n",
    "        Such a dataframe is returned by each of our models.\n",
    "\n",
    "        performance_lag (int) : Only use this many pieces of the latest data to compute\n",
    "        the performance metrics.\n",
    "\n",
    "        summarise (bool) : Whether to leave the performance metrics in their raw form,\n",
    "        i.e broken down by date (bool = False). Or whether to average all these\n",
    "        performance metrics to get a single set of of performance metrics (bool = True).\n",
    "\n",
    "    RETURNS:\n",
    "    ----------------\n",
    "        (pandas.core.frame.DataFrame): A dataframe of the performance metrics of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ts_dummy = df.copy()\n",
    "\n",
    "    series_col = ts_dummy.columns.values.tolist()[1]\n",
    "    preds_col = ts_dummy.columns.values.tolist()[2]\n",
    "\n",
    "    ts_dummy[series_col] = pd.to_numeric(ts_dummy[series_col])\n",
    "    ts_dummy[preds_col] = pd.to_numeric(ts_dummy[preds_col])\n",
    "\n",
    "    # add in the columns required for the first step of the calc\n",
    "    ts_dummy[\"perc error\"] = np.nan\n",
    "    ts_dummy[\"abs error\"] = np.nan\n",
    "    ts_dummy[\"sq error\"] = np.nan\n",
    "\n",
    "    ts_dummy[\"perc error\"] = (\n",
    "        abs(ts_dummy[series_col] - ts_dummy[preds_col]) / ts_dummy[series_col]\n",
    "    )\n",
    "    ts_dummy[\"abs error\"] = abs(ts_dummy[series_col] - ts_dummy[preds_col])\n",
    "    ts_dummy[\"sq error\"] = abs(ts_dummy[series_col] - ts_dummy[preds_col]) ** 2\n",
    "\n",
    "    ts_dummy = ts_dummy.tail(performance_lag)\n",
    "\n",
    "    if summarise is False:  # return raw performance metrics unless specifief otherwise\n",
    "        return ts_dummy\n",
    "    else:\n",
    "        # add in the columns required for the second step of the calc if summaraising is required.\n",
    "        mae = ts_dummy[\"abs error\"].mean()\n",
    "        rmse = (ts_dummy[\"sq error\"].mean()) ** 0.5\n",
    "        mape = ts_dummy[\"perc error\"].mean()\n",
    "\n",
    "        df_data = {\"model\": preds_col, \"mae\": mae, \"rmse\": rmse, \"mape\": mape}\n",
    "        metrics_df = pd.DataFrame(data=df_data, index=[0])\n",
    "\n",
    "        return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407fdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_merger(performance_dfs, metric):\n",
    "\n",
    "    \"\"\"\n",
    "    Merges together a list of performance dataframes, so models can be easily compared at a glance.\n",
    "\n",
    "    INPUTS:\n",
    "    ----------------\n",
    "        performance_dfs (list) : A list of performance dataframes. These dataframes are\n",
    "        the outputs of the performance_metrics function. See the performance_metrics\n",
    "        documentation for details.\n",
    "\n",
    "        metric (string) : The metric we wants to compare the models over. This can\n",
    "        either be 'mae', mape' or 'rmse'.\n",
    "\n",
    "    RETURNS:\n",
    "    ----------------\n",
    "        (pandas.core.frame.DataFrame): A dataframe of the performance metrics for each\n",
    "        of the different models.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    date_col = performance_dfs[0].columns.values.tolist()[0]\n",
    "    merged_df = performance_dfs[1][[date_col]]\n",
    "\n",
    "    for i in range(len(performance_dfs)):\n",
    "        model_name = performance_dfs[i].columns.values.tolist()[2]\n",
    "        trimmed_df = performance_dfs[i][[date_col, metric]].rename(\n",
    "            columns={metric: model_name}\n",
    "        )\n",
    "        merged_df = merged_df.merge(trimmed_df, how=\"inner\", on=[date_col])\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da462721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_df(model_performance_dfs, number_of_known_weeks, metric):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a list of dataframes, each of which summarises performance across a number of models.\n",
    "    These dataframes are assumed to have been constructed by first passing outputs of the\n",
    "    n_weeks_forecast function into the performance_metrics function. Therefore each\n",
    "    dataframe is based on knowing a specified number of weeks. This function returns a\n",
    "    dataframe whose rows are the number of known weeks and each column is a model. Each\n",
    "    value is the specified performance metric.\n",
    "\n",
    "    INPUTS:\n",
    "    ----------------\n",
    "        model_performance_dfs (list) : A list of dataframes, each of which is summarises\n",
    "        performance across a number of models.\n",
    "\n",
    "        number_of_known_weeks (int) : The number of known weeks for each dataframe in the list.\n",
    "        Note, the order of the number_of_known_weeks list should correspond to the ordering of\n",
    "        the dataframes. i.e. if the first dataframe is based on knowing 0 weeks of data, then\n",
    "        the first entry of number_of_known_weeks list should be 0.\n",
    "\n",
    "        metric (string) : The metric we wants to compare the models over.\n",
    "        This can either be 'mae', mape' or 'rmse'\n",
    "\n",
    "    RETURNS:\n",
    "    ----------------\n",
    "        (pandas.core.frame.DataFrame): A dataframe whose rows are the number of known weeks\n",
    "        and each column is a model. Each value is the specified performance metric.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metric_posn = model_performance_dfs[0].columns.get_loc(metric)\n",
    "    error_reduction_df = pd.DataFrame({\"number of known weeks\": number_of_known_weeks})\n",
    "    for i in range(len(model_performance_dfs[0])):\n",
    "        error_reduction_df[str(model_performance_dfs[0][\"model\"].iloc[i])] = np.nan\n",
    "\n",
    "    for col in range(len(error_reduction_df.columns.values.tolist()) - 1):\n",
    "        for row in range(4):\n",
    "            error_reduction_df.iloc[row, col + 1] = model_performance_dfs[row].iloc[\n",
    "                col, metric_posn\n",
    "            ]\n",
    "\n",
    "    return error_reduction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3003d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook performance_functions.ipynb to script\n",
      "[NbConvertApp] Writing 9076 bytes to performance_functions.py\n"
     ]
    }
   ],
   "source": [
    "# write all the above code to a py file but not this particular cell of code.\n",
    "\n",
    "!jupyter nbconvert --to script performance_functions.ipynb\n",
    "with open(\"performance_functions.py\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(\"performance_functions.py\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        if \"nbconvert --to script\" in line:\n",
    "            break\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd315d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
